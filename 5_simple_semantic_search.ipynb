{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719f0cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install -q langchain langchain-core google-genai python-dotenv langchain_google_genai pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405bc54",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209b6c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db194f98",
   "metadata": {},
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439a1e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./example_data/llmops.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f977966",
   "metadata": {},
   "source": [
    "### Split to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "befbea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b5eabb",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a68789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c954b",
   "metadata": {},
   "source": [
    "### Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3755f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6011c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3cde9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='From MLOps to LLMOps: Why Do We Need a\n",
      "New Framework?\n",
      "There is some overlap between MLOps and LLMOps; both\n",
      "deal with the operational lifecycles of ML models, after all.\n",
      "They also share common principles in terms of managing\n",
      "ML workflows. However, the two frameworks diverge in\n",
      "their primary focuses and objectives. While MLOps handles\n",
      "non-generative models (both language and computer\n",
      "vision), LLMOps deals with generative language models—\n",
      "and thus with mammoth levels of complexity. The\n",
      "complexity of these models owes not only to their scale and\n",
      "architecture but also to the unique processes involved in\n",
      "data engineering, domain adaptation, evaluation, and\n",
      "monitoring for them. The key distinctions are apparent in\n",
      "LLMs’ prediction transparency, latency, and memory and\n",
      "computational requirements.\n",
      "Perhaps the biggest difference is the shift in how end users\n",
      "consume these models. Non-generative ML models are\n",
      "predictive tools used for passive consumption, such as in' metadata={'producer': 'calibre 7.16.0', 'creator': 'calibre 7.16.0', 'creationdate': '2025-07-12T07:11:24+00:00', 'author': 'Abi Aryan', 'moddate': '2025-07-12T07:11:24+00:00', 'title': 'LLMOps (for Raymond Rhine)', 'source': './example_data/llmops.pdf', 'total_pages': 508, 'page': 47, 'page_label': '48', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"What is LLMOps?\"\n",
    ")\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e144d",
   "metadata": {},
   "source": [
    "### Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3073b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a36d5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def retriever(query: str) -> List[Document]:\n",
    "    return vector_store.similarity_search(query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88cb7788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='8bbe1189-f865-41af-ab8f-5cc2d4e51112', metadata={'producer': 'calibre 7.16.0', 'creator': 'calibre 7.16.0', 'creationdate': '2025-07-12T07:11:24+00:00', 'author': 'Abi Aryan', 'moddate': '2025-07-12T07:11:24+00:00', 'title': 'LLMOps (for Raymond Rhine)', 'source': './example_data/llmops.pdf', 'total_pages': 508, 'page': 47, 'page_label': '48', 'start_index': 0}, page_content='From MLOps to LLMOps: Why Do We Need a\\nNew Framework?\\nThere is some overlap between MLOps and LLMOps; both\\ndeal with the operational lifecycles of ML models, after all.\\nThey also share common principles in terms of managing\\nML workflows. However, the two frameworks diverge in\\ntheir primary focuses and objectives. While MLOps handles\\nnon-generative models (both language and computer\\nvision), LLMOps deals with generative language models—\\nand thus with mammoth levels of complexity. The\\ncomplexity of these models owes not only to their scale and\\narchitecture but also to the unique processes involved in\\ndata engineering, domain adaptation, evaluation, and\\nmonitoring for them. The key distinctions are apparent in\\nLLMs’ prediction transparency, latency, and memory and\\ncomputational requirements.\\nPerhaps the biggest difference is the shift in how end users\\nconsume these models. Non-generative ML models are\\npredictive tools used for passive consumption, such as in')],\n",
       " [Document(id='c8066f52-4203-4ff6-a2bb-98dcecfa96e6', metadata={'producer': 'calibre 7.16.0', 'creator': 'calibre 7.16.0', 'creationdate': '2025-07-12T07:11:24+00:00', 'author': 'Abi Aryan', 'moddate': '2025-07-12T07:11:24+00:00', 'title': 'LLMOps (for Raymond Rhine)', 'source': './example_data/llmops.pdf', 'total_pages': 508, 'page': 307, 'page_label': '308', 'start_index': 823}, page_content='degrade the quality of the model’s outputs. These\\ndegradations are often not immediately apparent, making\\nthem harder to detect and diagnose in real time.\\nThe challenge is compounded by the inherent\\nnondeterminism of LLMs: given the same input, the model\\nmay generate different outputs across runs, due to\\nsampling methods and stochastic token prediction. This\\nvariability makes it difficult to reproduce prompt\\nregressions consistently, posing a significant barrier to\\ntraditional debugging approaches.\\nTo manage this complexity, robust evaluation systems must\\nintegrate detailed prompt versioning and logging\\ncapabilities. Tracking changes at a granular level is\\nessential, as is supporting prompt diffs that highlight\\nexactly what was modified between versions. By correlating\\nthese prompt changes with measurable metrics, such as')]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.batch(\n",
    "    [\n",
    "        \"Why do we need LLMOps?\",\n",
    "        \"How can we manage versioning of prompts?\",\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
